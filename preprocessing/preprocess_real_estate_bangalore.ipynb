{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4ee448",
   "metadata": {},
   "source": [
    " **Step 1: Import Libraries**\n",
    "\n",
    "We import required libraries for data processing, visualization, preprocessing, and model building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d1dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfcb8aa",
   "metadata": {},
   "source": [
    "**Step 2: Load Dataset**\n",
    "\n",
    "We load the dataset from the given path and preview its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"../datasets/Bangalore.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38d237",
   "metadata": {},
   "source": [
    "**Step 3: Explore Data**\n",
    "\n",
    "We check dataset shape, info, descriptive statistics, column names, and missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81325ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.info()\n",
    "df.describe()\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd747f5e",
   "metadata": {},
   "source": [
    "**Step 4: Handle Missing Values**\n",
    "\n",
    "We remove rows with missing data and drop duplicate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65851ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b64da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop_duplicates(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of each column\n",
    "df_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typed = df_cleaned.copy()\n",
    "df_typed['Location'] = df_typed['Location'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d326a6e",
   "metadata": {},
   "source": [
    "**Step 5: Outlier Visualization**\n",
    "\n",
    "We visualize potential outliers in key numeric features using boxplots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bea296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting Outliers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bbe4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63db282",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=df_typed['Price'])\n",
    "plt.title(\"Boxplot of Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dedde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=df_typed['Area'])\n",
    "plt.title(\"Boxplot of Area\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_outliers_iqr(df, column):\n",
    "   # Q1 = df[column].quantile(0.25)\n",
    "   # Q3 = df[column].quantile(0.75)\n",
    "    #IQR = Q3 - Q1\n",
    "    #lower_bound = Q1 - 1.5 * IQR\n",
    "    #upper_bound = Q3 + 1.5 * IQR\n",
    "    #return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7c3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_outliers = remove_outliers_iqr(df_typed, 'Price')\n",
    "#df_no_outliers = remove_outliers_iqr(df_no_outliers, 'Area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa21c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing the EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121ec2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_no_outliers' not in locals():\n",
    "    df_no_outliers = df_typed.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40776c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of Price\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_no_outliers['Price'], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Price\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Area\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_no_outliers['Area'], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Area\")\n",
    "plt.xlabel(\"Area\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882cc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of top 15 locations\n",
    "plt.figure(figsize=(12,6))\n",
    "top_locations = df_no_outliers['Location'].value_counts().head(15)\n",
    "\n",
    "sns.barplot(x=top_locations.values, y=top_locations.index, palette=\"viridis\")\n",
    "plt.title(\"Top 15 Locations by Number of Listings\")\n",
    "plt.xlabel(\"Number of Properties\")\n",
    "plt.ylabel(\"Location\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "top_loc = df_no_outliers['Location'].value_counts().head(10).index\n",
    "\n",
    "# Filter only top locations to make it readable\n",
    "sns.boxplot(x='Location', y='Price', data=df_no_outliers[df_no_outliers['Location'].isin(top_loc)])\n",
    "plt.title(\"Price Distribution Across Top 10 Locations\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are doing this to see how area affects price\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Area', y='Price', data=df_no_outliers, hue='Location', palette='cool', alpha=0.6)\n",
    "plt.title(\"Area vs Price (Colored by Location)\")\n",
    "plt.xlabel(\"Area (sq ft)\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0350cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluding the non-numeric values\n",
    "# Only select numeric columns for correlation\n",
    "numeric_df = df_no_outliers.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Now generate the correlation heatmap\n",
    "plt.figure(figsize=(18, 15))\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df_no_outliers.drop('Price', axis=1)\n",
    "y = df_no_outliers['Price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a94b53",
   "metadata": {},
   "source": [
    "**Step 6: Scale Numeric Columns**\n",
    "\n",
    "We identify continuous numeric columns and prepare a scaler to normalize them.  \n",
    "Scaling ensures all features have a similar range, which improves model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66523ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Target column name\n",
    "target_col = 'Price'\n",
    "\n",
    "# Figure out which numeric columns should be scaled (exclude target + binary/dummy cols)\n",
    "num_cols_all = df_no_outliers.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if target_col in num_cols_all:\n",
    "    num_cols_all.remove(target_col)\n",
    "\n",
    "# Exclude likely dummy/binary columns (nunique <= 2)\n",
    "cont_cols = [c for c in num_cols_all if df_no_outliers[c].nunique() > 10]\n",
    "\n",
    "print(\"Continuous numeric columns to scale:\", cont_cols)\n",
    "\n",
    "# Helper to scale AFTER we split (so we fit only on training data)\n",
    "def fit_transform_scaler(X_train, X_test, cols):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    if cols:\n",
    "        scaler.fit(X_train[cols])\n",
    "        X_train_scaled[cols] = scaler.transform(X_train[cols])\n",
    "        X_test_scaled[cols] = scaler.transform(X_test[cols])\n",
    "    return scaler, X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebb421",
   "metadata": {},
   "source": [
    "**Step 7: Train–Test Split**\n",
    "\n",
    "We split the dataset into training and testing sets, then apply scaling only on the training data to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0888e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Encode Categorical Variables & Train–Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# One-hot encode 'Location' from the cleaned DataFrame\n",
    "df_encoded = pd.get_dummies(df_no_outliers, columns=['Location'], drop_first=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_encoded.drop('Price', axis=1)\n",
    "y = df_encoded['Price']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f253209",
   "metadata": {},
   "source": [
    "## Step 8: Feature Scaling (Avoiding Data Leakage)\n",
    "We scale the features so that all variables contribute equally to the model and avoid bias from different value ranges.  \n",
    "Scaling is fitted only on training data to prevent data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065add7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler only on training data, then transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63840205",
   "metadata": {},
   "source": [
    "## Step 9: Model Training (Linear Regression)\n",
    "We train the Linear Regression model using the scaled training data so it can learn the relationship between features and target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47230057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize and train model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b93e0e",
   "metadata": {},
   "source": [
    "## Step 10: Model Evaluation\n",
    "We evaluate the trained Linear Regression model on the test data to check its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c983c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361503c9",
   "metadata": {},
   "source": [
    "## Step 11: Model Improvement\n",
    "\n",
    "In this step, we improve our model by using a **Random Forest Regressor**.  \n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their results to improve accuracy and reduce overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f084f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Step 11: Model Improvement\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Mean Squared Error:\", mse_rf)\n",
    "print(\"Random Forest R² Score:\", r2_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9bc391",
   "metadata": {},
   "source": [
    "### Step 11 Output:\n",
    "\n",
    "- **Random Forest Mean Squared Error:** 220,024,586,218,197.84  \n",
    "- **Random Forest R² Score:** 0.25936823463030556  \n",
    "\n",
    "The R² score improved compared to the previous model (0.24 → 0.26), showing that the Random Forest is capturing more patterns in the data.  \n",
    "While the MSE is still high, this is expected given the large scale of the target values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5e472",
   "metadata": {},
   "source": [
    "### Step 12: Hyperparameter Tuning for Random Forest\n",
    "\n",
    "We use `GridSearchCV` to find the optimal combination of hyperparameters for the Random Forest model, aiming to improve the R² score and reduce the Mean Squared Error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=5, n_jobs=-1, scoring='r2', verbose=2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R² Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set with the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf_best = best_rf.predict(X_test)\n",
    "\n",
    "mse_rf_best = mean_squared_error(y_test, y_pred_rf_best)\n",
    "r2_rf_best = r2_score(y_test, y_pred_rf_best)\n",
    "\n",
    "print(f\"Tuned Random Forest Mean Squared Error: {mse_rf_best}\")\n",
    "print(f\"Tuned Random Forest R² Score: {r2_rf_best}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ec615",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Results\n",
    "- **Best Parameters**:  \n",
    "  - `max_depth`: 30  \n",
    "  - `max_features`: 'sqrt'  \n",
    "  - `min_samples_leaf`: 1  \n",
    "  - `min_samples_split`: 10  \n",
    "  - `n_estimators`: 100  \n",
    "\n",
    "- **Performance**:  \n",
    "  - **Best R² Score**: 0.25165614871803543  \n",
    "  - **Tuned Random Forest Mean Squared Error**: 200471448148561.44  \n",
    "  - **Tuned Random Forest R² Score**: 0.3251866752688929  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da91586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert GridSearchCV results to DataFrame\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select only important columns\n",
    "results_df = results_df[[\"param_max_depth\", \"param_max_features\", \"param_min_samples_leaf\", \n",
    "                          \"param_min_samples_split\", \"param_n_estimators\", \"mean_test_score\"]]\n",
    "\n",
    "# Sort by best score\n",
    "results_df = results_df.sort_values(by=\"mean_test_score\", ascending=False)\n",
    "\n",
    "# Convert to Markdown format\n",
    "print(results_df.to_markdown(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
